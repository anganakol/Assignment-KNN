{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMd2ySIJAzml1O271s6949h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Theory\n","\n","1. What is K-Nearest Neighbors (KNN) and how does it work?\n","ans- K-Nearest Neighbors (KNN) is a non-parametric, lazy learning algorithm used for both classification and regression tasks.\n","\n","  How it works:\n","  Training Phase (Lazy): There is no explicit training phase. The algorithm simply stores the entire training dataset.\n","\n","  Prediction Phase:\n","  To classify a new data point, KNN finds the 'k' closest data points (neighbors) in the training dataset based on a chosen distance metric (e.g., Euclidean, Manhattan).\n","  For classification, the new data point is assigned the class label that is most frequent among its 'k' nearest neighbors (majority vote).\n","  For regression, the new data point is assigned the average (or weighted average) of the target values of its 'k' nearest neighbors.\n","\n","2. What is the difference between KNN Classification and KNN Regression?\n","\n","ans- The core difference lies in the type of output predicted and how the 'k' nearest neighbors' values are aggregated:\n","\n","KNN Classification: Predicts a discrete class label. The output is determined by a majority vote among the class labels of the 'k' nearest neighbors. For example, if among 5 neighbors, 3 are 'Class A' and 2 are 'Class B', the new point is classified as 'Class A'.\n","KNN Regression: Predicts a continuous numerical value. The output is typically the mean (average) of the target values of the 'k' nearest neighbors. For example, if the target values of 5 neighbors are [10, 12, 11, 13, 9], the predicted value for the new point would be their average (11).\n","\n","3. What is the role of the distance metric in KNN?\n","\n","ans- The distance metric is fundamental to KNN because it defines \"closeness\" or \"similarity\" between data points. It determines how the 'k' nearest neighbors are identified. Different distance metrics can lead to different sets of neighbors, and therefore, different predictions. Common distance metrics include:\n","\n","Euclidean Distance: The straight-line distance between two points in a Euclidean space (most common).\n","Manhattan Distance (City Block Distance): The sum of the absolute differences of their coordinates.\n","Minkowski Distance: A generalization of Euclidean and Manhattan distances.\n","\n","4. What is the Curse of Dimensionality in KNN?\n","\n","ans- The \"Curse of Dimensionality\" refers to various problems that arise when working with high-dimensional data. In the context of KNN:\n","\n","As the number of features (dimensions) increases, the volume of the feature space grows exponentially.\n","Data points become increasingly sparse, meaning that even \"close\" neighbors might be far apart in high dimensions.\n","The concept of \"distance\" becomes less meaningful, as distances between data points tend to converge, making it difficult to distinguish between true neighbors and non-neighbors.\n","This leads to KNN becoming less effective and computationally more expensive in high-dimensional spaces.\n","\n","5. How can we choose the best value of K in KNN?\n","\n","ans- Choosing an optimal 'K' is crucial for KNN performance. There's no single best 'K' for all datasets, but common approaches include:\n","\n","Trial and Error / Grid Search: Test different values of 'K' (e.g., odd numbers like 1, 3, 5, 7, 9...) and evaluate the model's performance (e.g., using accuracy for classification or MSE for regression) on a validation set or using cross-validation.\n","Cross-Validation: This is the most robust method. The dataset is split into multiple folds. The model is trained on some folds and tested on the remaining fold, rotating through all folds. This provides a more reliable estimate of performance for each 'K'.\n","Rule of thumb: Sometimes, K=\n","N\n","\n","â€‹\n","  (where N is the number of samples) is suggested, but this is a rough guideline.\n","Consider odd K for classification: For binary or multi-class classification, using an odd 'K' helps avoid ties in majority voting.\n","\n","6. What are KD Tree and Ball Tree in KNN?\n","\n","ans- KD Trees and Ball Trees are data structures designed to speed up the process of finding nearest neighbors, especially in datasets with many data points or higher dimensions.\n","\n","KD Tree (K-Dimensional Tree): A binary tree that partitions the data space by recursively splitting it along one of the feature axes. It's efficient for low-to-medium dimensional data.\n","Ball Tree: A binary tree where each node represents a \"hyper-sphere\" (ball) containing a subset of data points. It's often more efficient than KD Trees in higher-dimensional spaces and for arbitrary distance metrics because it doesn't rely on axis-aligned splits.\n","\n","7. When should you use KD Tree vs. Ball Tree?\n","\n","ans- KD Tree: Generally preferred for lower-dimensional data (typically less than 20 dimensions) and when using Euclidean distance (or other L_p norms). It's simpler to implement and often faster for these scenarios.\n","Ball Tree: Generally preferred for higher-dimensional data (above 20 dimensions where KD Trees become inefficient due to the curse of dimensionality) or when using non-Euclidean distance metrics where axis-aligned splits are less effective.\n","\n","8. What are the disadvantages of KNN?\n","\n","ans- Computationally Expensive (Prediction Phase): For large datasets, finding the 'k' nearest neighbors for each new data point can be very slow, as it requires calculating distances to all training points.\n","Memory Intensive: It needs to store the entire training dataset in memory, which can be an issue for very large datasets.\n","Sensitive to Irrelevant Features: Irrelevant or noisy features can disproportionately influence distance calculations, leading to poor performance.\n","Sensitive to Feature Scaling: Features with larger scales will have a greater impact on distance calculations than features with smaller scales.\n","Doesn't work well with High-Dimensional Data (Curse of Dimensionality): Distances become less meaningful, and computational cost increases.\n","Imbalanced Data: If one class is heavily imbalanced, the majority class might dominate the 'k' neighbors, leading to biased predictions for the minority class.\n","\n","9. How does feature scaling affect KNN?\n","\n","ans- Feature scaling is critical for KNN. Since KNN relies on distance calculations to find neighbors:\n","\n","Features with larger numerical ranges or higher magnitudes will inherently have a disproportionately larger impact on the distance calculation compared to features with smaller ranges.\n","This can lead to features with smaller ranges being effectively ignored, even if they are highly predictive.\n","Solution: Techniques like Standardization (Z-score normalization) or Normalization (Min-Max scaling) should be applied to bring all features to a similar scale before applying KNN.\n","\n","10. How does KNN handle missing values in a dataset?\n","ans- KNN itself does not inherently handle missing values. If a data point has missing values, its distance to other points cannot be accurately calculated using standard distance metrics. Common strategies to deal with missing values before applying KNN include:\n","\n","Imputation: Replacing missing values with estimated values, such as:\n","Mean, median, or mode of the feature.\n","Using more sophisticated imputation methods (e.g., K-Nearest Neighbor Imputation, where missing values are imputed based on the non-missing values of the 'k' nearest neighbors).\n","Exclusion: Removing rows (data points) or columns (features) with missing values. This is only feasible if there are very few missing values.\n","Specific distance metrics: Some specialized distance metrics can be used that are designed to handle missing values, but these are less common in general KNN implementations.\n","Principal Component Analysis (PCA)\n","\n","11. What is PCA (Principal Component Analysis)?\n","\n","ans- Principal Component Analysis (PCA) is an unsupervised linear dimensionality reduction technique. Its primary goal is to transform a dataset with potentially correlated variables into a new set of uncorrelated variables called Principal Components (PCs), while retaining as much of the original variance as possible.\n","\n","12. How does PCA work?\n","\n","ans- PCA works by:\n","\n","Standardizing the Data: (Optional but recommended) Scaling features to have zero mean and unit variance, as PCA is sensitive to feature scaling.\n","Calculating the Covariance Matrix: This matrix describes the relationships (covariance) between all pairs of features.\n","Computing Eigenvalues and Eigenvectors:\n","Eigenvectors represent the directions (axes) of maximum variance in the data. These are the Principal Components.\n","Eigenvalues represent the magnitude of variance along each eigenvector. A higher eigenvalue indicates that its corresponding eigenvector captures more variance.\n","Selecting Principal Components: The principal components are ranked by their corresponding eigenvalues in descending order. You choose the top 'k' eigenvectors (where 'k' is the desired number of dimensions) that capture a sufficient amount of variance.\n","Transforming the Data: Project the original data onto the subspace defined by the selected principal components. This creates a new, lower-dimensional representation of the data.\n","\n","13. What is the geometric intuition behind PCA?\n","\n","ans- Geometrically, PCA finds a new coordinate system (a set of orthogonal axes) for the data.\n","\n","The first principal component is the direction along which the data varies the most (the line that best fits the data in a way that minimizes the perpendicular distances of points to the line).\n","The second principal component is orthogonal to the first principal component and captures the next largest amount of variance, and so on.\n","Essentially, PCA rotates the existing data axes to align with the directions of maximum variance in the data, thereby \"spreading out\" the data as much as possible along these new axes. This allows for projection onto a lower-dimensional space while preserving the most significant information (variance).\n","\n","14. What is the difference between Feature Selection and Feature Extraction?\n","\n","ans- Feature Selection: This process involves choosing a subset of the original features from the dataset that are most relevant to the prediction task. It aims to eliminate redundant or irrelevant features. The selected features retain their original meaning. Examples: Variance Thresholding, Recursive Feature Elimination, correlation-based methods.\n","Feature Extraction: This process involves transforming the original features into a new, smaller set of features (components) while retaining most of the important information. The new features are often combinations or projections of the original features and may not have a direct, interpretable meaning like the original features. PCA is a prime example of feature extraction.\n","\n","15. What are Eigenvalues and Eigenvectors in PCA?\n","\n","ans- In the context of PCA:\n","\n","Eigenvectors: These are the directions or axes of the new feature space. They represent the principal components. Each eigenvector points in a direction along which the data exhibits maximum variance. They are orthogonal to each other.\n","Eigenvalues: Each eigenvector has a corresponding eigenvalue. The eigenvalue quantifies the amount of variance captured by its corresponding eigenvector (principal component). Larger eigenvalues indicate that their corresponding eigenvectors capture more of the data's variance, making them more significant.\n","\n","16. How do you decide the number of components to keep in PCA?\n","\n","ans- Deciding the number of components (k) to keep is crucial:\n","\n","Scree Plot: Plot the eigenvalues in descending order. Look for an \"elbow\" point where the explained variance drops significantly. The components before the elbow are usually retained.\n","Explained Variance Ratio: Calculate the cumulative sum of the explained variance ratio for each principal component. Choose the number of components that explain a certain percentage of the total variance (e.g., 95% or 99%). This is often the most common and systematic approach.\n","Kaiser's Rule: Keep only principal components whose eigenvalues are greater than 1. (Less common in practice for high-dimensional data).\n","Downstream Task Performance: If PCA is used as a preprocessing step for another model, evaluate the performance of that model with different numbers of components.\n","\n","17. Can PCA be used for classification?\n","\n","ans- No, PCA itself is not a classification algorithm. PCA is an unsupervised dimensionality reduction technique. It transforms the data into a lower-dimensional space.\n","However, PCA is very frequently used as a preprocessing step for classification algorithms. By reducing the dimensionality of the input data, PCA can:\n","\n","Reduce computational cost and training time for subsequent classification models.\n","Mitigate the curse of dimensionality, potentially improving the performance of classifiers (like KNN) in high-dimensional spaces.\n","Help visualize high-dimensional data by reducing it to 2 or 3 components. So, you would typically run PCA, then feed the PCA-transformed data into a classifier like Logistic Regression, SVM, or KNN.\n","\n","18. What are the limitations of PCA?\n","\n","ans- Linearity Assumption: PCA assumes linear relationships between variables. It may not perform well if the data has complex non-linear structures.\n","Sensitivity to Scaling: PCA is sensitive to the scaling of features. Features with larger variances will have a disproportionately larger influence on the principal components. Data standardization is often required.\n","Loss of Interpretability: The new principal components are linear combinations of the original features, which can make them difficult to interpret in terms of the original domain.\n","Information Loss: While it tries to retain maximum variance, some information is always lost during dimensionality reduction.\n","Outlier Sensitivity: PCA is sensitive to outliers, which can significantly affect the calculated principal components.\n","Unsupervised: It does not consider the class labels (target variable) during the transformation, which means it might not always find the optimal components for a supervised task.\n","\n","19. How do KNN and PCA complement each other?\n","\n","ans- KNN and PCA are often used together, with PCA serving as a powerful preprocessing step for KNN:\n","\n","Addressing the Curse of Dimensionality for KNN: PCA reduces the number of features, mitigating the negative effects of high dimensionality on KNN (computational cost, sparsity, meaningful distance).\n","Improving KNN Performance: By removing noise and irrelevant features (implicitly through variance maximization), PCA can often improve the accuracy and efficiency of KNN, especially in high-dimensional datasets.\n","Reducing Computational Load: With fewer dimensions, KNN's distance calculations become faster and less memory-intensive.\n","Enhancing Interpretability (Visualization): PCA can reduce data to 2 or 3 dimensions, making it possible to visualize clusters or relationships in the data before applying KNN.\n","\n","20. How does KNN handle missing values in a dataset?\n","\n","ans- KNN does not inherently handle missing values. If a data point has missing values, its distance to other points cannot be accurately calculated using standard distance metrics. Common strategies to deal with missing values before applying KNN include:\n","\n","Imputation: Replacing missing values with estimated values, such as:\n","Mean, median, or mode of the feature.\n","Using more sophisticated imputation methods (e.g., K-Nearest Neighbor Imputation, where missing values are imputed based on the non-missing values of the 'k' nearest neighbors).\n","Exclusion: Removing rows (data points) or columns (features) with missing values. This is only feasible if there are very few missing values.\n","Specific distance metrics: Some specialized distance metrics can be used that are designed to handle missing values, but these are less common in general KNN implementations.\n","What are the key differences between PCA and Linear Discriminant Analysis (LDA)?\n","Both PCA and LDA are dimensionality reduction techniques, but they have fundamental differences:\n","\n","| Feature               | Principal Component Analysis (PCA)                               | Linear Discriminant Analysis (LDA)                                |\n","| :-------------------- | :--------------------------------------------------------------- | :---------------------------------------------------------------- |\n","| Type of Algorithm | Unsupervised                                                     | Supervised (requires class labels)                                |\n","| Goal | Maximize variance in the projected data. Find directions of max variance. | Maximize class separability. Find directions that best separate classes. |\n","| Information Used | Only feature data (X). Ignores class labels.                     | Feature data (X) AND class labels (y).                           |\n","| Objective | Transform data to a new space where components are uncorrelated. | Project data to maximize the ratio of between-class variance to within-class variance. |\n","| Number of Comp. | Up to min(n_samples - 1, n_features)                           | At most (number of classes - 1)                                 |\n","| Use Cases | General dimensionality reduction, noise reduction, visualization.  | Classification preprocessing, feature extraction for classification. |\n","| Limitations | Assumes linearity, sensitive to scaling, ignores class info.       | Assumes Gaussian distribution and equal covariance matrices per class. Less effective if classes are not well-separated linearly. |\n","| Interpretation | Components are difficult to interpret.                           | Components are discriminant functions, related to class separation. |"],"metadata":{"id":"ENqODjXoD-3h"}},{"cell_type":"code","source":[],"metadata":{"id":"-zt3MTHcEuOG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Practical"],"metadata":{"id":"7RK5yc_6_g-6"}},{"cell_type":"code","source":["pip install scikit-learn numpy matplotlib seaborn"],"metadata":{"id":"cxN7D_QX_s1H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.datasets import load_iris, make_regression, make_classification\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n","from sklearn.metrics import accuracy_score, mean_squared_error\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from matplotlib.colors import ListedColormap"],"metadata":{"id":"Blm7v96Q_uvV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["21: Train a KNN Classifier on the Iris dataset and print model accuracy."],"metadata":{"id":"PGmxk77a_kZB"}},{"cell_type":"code","source":["# Load Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Train a KNN Classifier (default k=5)\n","knn_classifier = KNeighborsClassifier(n_neighbors=5)\n","knn_classifier.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = knn_classifier.predict(X_test)\n","\n","# Calculate and print accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Task 21: KNN Classifier Accuracy on Iris Dataset: {accuracy:.4f}\")"],"metadata":{"id":"ZTZracn6_1t7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["22: Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE)."],"metadata":{"id":"sbDCgs_6_4SO"}},{"cell_type":"code","source":["# Generate a synthetic regression dataset\n","X_reg, y_reg = make_regression(n_samples=100, n_features=1, noise=20, random_state=42)\n","\n","# Split data into training and testing sets\n","X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n","    X_reg, y_reg, test_size=0.3, random_state=42\n",")\n","\n","# Train a KNN Regressor (default k=5)\n","knn_regressor = KNeighborsRegressor(n_neighbors=5)\n","knn_regressor.fit(X_train_reg, y_train_reg)\n","\n","# Make predictions\n","y_pred_reg = knn_regressor.predict(X_test_reg)\n","\n","# Calculate and print Mean Squared Error\n","mse = mean_squared_error(y_test_reg, y_pred_reg)\n","print(f\"\\nTask 22: KNN Regressor Mean Squared Error (MSE): {mse:.4f}\")\n","\n","# Optional: Visualize the regression\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X_test_reg, y_test_reg, color='blue', label='Actual values')\n","plt.scatter(X_test_reg, y_pred_reg, color='red', label='Predicted values')\n","plt.title('Task 22: KNN Regression on Synthetic Dataset')\n","plt.xlabel('Feature')\n","plt.ylabel('Target')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"J5iu2M5A_6RV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["23: Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy."],"metadata":{"id":"gEv8th7k_7pf"}},{"cell_type":"code","source":["# Use the Iris dataset (already loaded in Task 21)\n","# X_train, X_test, y_train, y_test are already defined\n","\n","# KNN with Euclidean distance\n","knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n","knn_euclidean.fit(X_train, y_train)\n","y_pred_euclidean = knn_euclidean.predict(X_test)\n","accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n","\n","# KNN with Manhattan distance\n","knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n","knn_manhattan.fit(X_train, y_train)\n","y_pred_manhattan = knn_manhattan.predict(X_test)\n","accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n","\n","print(f\"\\nTask 23: KNN Accuracy (Euclidean Distance): {accuracy_euclidean:.4f}\")\n","print(f\"Task 23: KNN Accuracy (Manhattan Distance): {accuracy_manhattan:.4f}\")\n","\n","if accuracy_euclidean > accuracy_manhattan:\n","    print(\"           Euclidean distance performed better.\")\n","elif accuracy_manhattan > accuracy_euclidean:\n","    print(\"           Manhattan distance performed better.\")\n","else:\n","    print(\"           Both distances performed equally well.\")"],"metadata":{"id":"vq5gGmPj_7co"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["24: Train a KNN Classifier with different values of K and visualize decision boundaries."],"metadata":{"id":"40F5FAyBAAOW"}},{"cell_type":"code","source":["# Use only the first two features for visualization\n","X_2d = X[:, :2] # Sepal Length, Sepal Width\n","y_2d = y\n","\n","# Split data for 2D visualization\n","X_train_2d, X_test_2d, y_train_2d, y_test_2d = train_test_split(X_2d, y_2d, test_size=0.3, random_state=42)\n","\n","# Define a color map for the classes\n","cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n","cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n","\n","plt.figure(figsize=(15, 5))\n","\n","# Iterate over different K values\n","k_values = [1, 5, 15]\n","for i, k in enumerate(k_values):\n","    knn = KNeighborsClassifier(n_neighbors=k)\n","    knn.fit(X_train_2d, y_train_2d)\n","\n","    # Plot the decision boundary. For that, we will assign a color to each\n","    # point in the mesh [x_min, x_max]x[y_min, y_max].\n","    x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\n","    y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n","                         np.arange(y_min, y_max, 0.02))\n","    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n","\n","    # Put the result into a color plot\n","    Z = Z.reshape(xx.shape)\n","    plt.subplot(1, len(k_values), i + 1)\n","    plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')\n","\n","    # Plot also the training points\n","    plt.scatter(X_train_2d[:, 0], X_train_2d[:, 1], c=y_train_2d, cmap=cmap_bold,\n","                edgecolor='k', s=20, label='Training points')\n","    plt.scatter(X_test_2d[:, 0], X_test_2d[:, 1], c=y_test_2d, cmap=cmap_bold,\n","                edgecolor='k', s=60, marker='X', label='Test points')\n","    plt.xlim(xx.min(), xx.max())\n","    plt.ylim(yy.min(), yy.max())\n","    plt.title(f\"Task 24: K-NN Classifier (K={k})\")\n","    plt.xlabel(iris.feature_names[0])\n","    plt.ylabel(iris.feature_names[1])\n","\n","plt.suptitle(\"Decision Boundaries for different K values\", y=1.02, fontsize=16)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"NdG9ImlWABhI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["25: Apply Feature Scaling before training a KNN model and compare results with unscaled data."],"metadata":{"id":"m8vI55MeAC4z"}},{"cell_type":"code","source":["# Use the Iris dataset (X, y from Task 21)\n","# X_train, X_test, y_train, y_test are already defined\n","\n","# KNN on unscaled data (from Task 21)\n","knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n","knn_unscaled.fit(X_train, y_train)\n","y_pred_unscaled = knn_unscaled.predict(X_test)\n","accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n","print(f\"\\nTask 25: KNN Accuracy (Unscaled Data): {accuracy_unscaled:.4f}\")\n","\n","# Apply Feature Scaling\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# KNN on scaled data\n","knn_scaled = KNeighborsClassifier(n_neighbors=5)\n","knn_scaled.fit(X_train_scaled, y_train)\n","y_pred_scaled = knn_scaled.predict(X_test_scaled)\n","accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n","print(f\"Task 25: KNN Accuracy (Scaled Data): {accuracy_scaled:.4f}\")\n","\n","if accuracy_scaled > accuracy_unscaled:\n","    print(\"           Scaling improved accuracy.\")\n","elif accuracy_unscaled > accuracy_scaled:\n","    print(\"           Scaling did not improve accuracy (or slightly reduced).\")\n","else:\n","    print(\"           Scaling had no effect on accuracy.\")"],"metadata":{"id":"A79JZKbdAEPc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["26: Train a PCA model on synthetic data and print the explained variance ratio for each component."],"metadata":{"id":"PxUmBlzFAFo_"}},{"cell_type":"code","source":["# Generate a synthetic dataset with more features\n","X_synthetic_pca, y_synthetic_pca = make_classification(\n","    n_samples=100, n_features=10, n_informative=5, n_redundant=2, random_state=42\n",")\n","\n","# Train a PCA model\n","# We'll reduce to fewer components than original features, e.g., 5\n","pca = PCA(n_components=5)\n","pca.fit(X_synthetic_pca)\n","\n","# Print explained variance ratio\n","print(f\"\\nTask 26: Explained Variance Ratio for each PCA component (synthetic data):\")\n","for i, ratio in enumerate(pca.explained_variance_ratio_):\n","    print(f\"  Component {i+1}: {ratio:.4f}\")\n","\n","print(f\"Total Explained Variance (first {pca.n_components} components): {pca.explained_variance_ratio_.sum():.4f}\")"],"metadata":{"id":"XZeFUHpUAHMe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["27: Apply PCA before training a KNN Classifier and compare accuracy with and without PCA."],"metadata":{"id":"639SCvBAAJQT"}},{"cell_type":"code","source":["# Use the Iris dataset (X, y from Task 21)\n","# X_train, X_test, y_train, y_test are already defined\n","\n","# --- KNN without PCA (from Task 21) ---\n","# knn_no_pca = KNeighborsClassifier(n_neighbors=5)\n","# knn_no_pca.fit(X_train, y_train)\n","# y_pred_no_pca = knn_no_pca.predict(X_test)\n","# accuracy_no_pca = accuracy_score(y_test, y_pred_no_pca)\n","# print(f\"\\nTask 27: KNN Accuracy (Without PCA): {accuracy_no_pca:.4f}\")\n","# Using previous result for direct comparison\n","print(f\"\\nTask 27: KNN Accuracy (Without PCA): {accuracy:.4f} (from Task 21)\")\n","\n","\n","# --- KNN with PCA ---\n","# Apply PCA (e.g., reduce to 2 components for visualization or 95% variance)\n","# For Iris (4 features), let's reduce to 2 or 3 components to see effect\n","pca_knn = PCA(n_components=2) # Reduce 4 features to 2\n","X_train_pca = pca_knn.fit_transform(X_train)\n","X_test_pca = pca_knn.transform(X_test)\n","\n","knn_with_pca = KNeighborsClassifier(n_neighbors=5)\n","knn_with_pca.fit(X_train_pca, y_train)\n","y_pred_with_pca = knn_with_pca.predict(X_test_pca)\n","accuracy_with_pca = accuracy_score(y_test, y_pred_with_pca)\n","print(f\"Task 27: KNN Accuracy (With PCA, n_components={pca_knn.n_components}): {accuracy_with_pca:.4f}\")\n","\n","if accuracy_with_pca > accuracy: # Comparing with accuracy from Task 21\n","    print(\"           PCA improved accuracy.\")\n","elif accuracy > accuracy_with_pca:\n","    print(\"           PCA reduced accuracy.\")\n","else:\n","    print(\"           PCA had no effect on accuracy.\")\n","\n","# Optional: Visualize PCA-transformed data (2D)\n","plt.figure(figsize=(8, 6))\n","sns.scatterplot(x=X_train_pca[:, 0], y=X_train_pca[:, 1], hue=y_train, palette='viridis', legend='full')\n","plt.title('Task 27: PCA Transformed Iris Data (2 Components)')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.show()"],"metadata":{"id":"SywHwK_lAJqH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["28: Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV."],"metadata":{"id":"P39SFxZzAKsh"}},{"cell_type":"code","source":["# Use the Iris dataset (X, y from Task 21)\n","# X_train, X_test, y_train, y_test are already defined\n","\n","# Define the parameter grid\n","param_grid = {'n_neighbors': np.arange(1, 21)} # Test K from 1 to 20\n","\n","# Initialize GridSearchCV\n","grid_search = GridSearchCV(\n","    KNeighborsClassifier(),\n","    param_grid,\n","    cv=5, # 5-fold cross-validation\n","    scoring='accuracy',\n","    n_jobs=-1 # Use all available CPU cores\n",")\n","\n","# Fit GridSearchCV to the training data\n","grid_search.fit(X_train, y_train)\n","\n","# Print the best parameters and best score\n","print(f\"\\nTask 28: Best K for KNN Classifier (GridSearchCV): {grid_search.best_params_['n_neighbors']}\")\n","print(f\"Task 28: Best Cross-validation Accuracy (GridSearchCV): {grid_search.best_score_:.4f}\")\n","\n","# Evaluate on the test set with the best estimator\n","best_knn = grid_search.best_estimator_\n","y_pred_tuned = best_knn.predict(X_test)\n","accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n","print(f\"Task 28: Test Accuracy with Best K: {accuracy_tuned:.4f}\")"],"metadata":{"id":"siyZiaN8AMTd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["29: Train a KNN Classifier and check the number of misclassified samples."],"metadata":{"id":"pirY6Gv9AOmk"}},{"cell_type":"code","source":["# Use the Iris dataset (X, y from Task 21)\n","# X_train, X_test, y_train, y_test are already defined\n","\n","# Train a KNN Classifier (default k=5)\n","knn_misclassified = KNeighborsClassifier(n_neighbors=5)\n","knn_misclassified.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred_misclassified = knn_misclassified.predict(X_test)\n","\n","# Calculate misclassified samples\n","misclassified_samples = np.sum(y_pred_misclassified != y_test)\n","total_samples = len(y_test)\n","\n","print(f\"\\nTask 29: Total Test Samples: {total_samples}\")\n","print(f\"Task 29: Number of Misclassified Samples: {misclassified_samples}\")\n","print(f\"Task 29: Classification Error Rate: {misclassified_samples / total_samples:.4f}\")"],"metadata":{"id":"RvGSt9hyAPCN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["30: Train a PCA model and visualize the cumulative explained variance."],"metadata":{"id":"Gsu4I1VdAQJg"}},{"cell_type":"code","source":["# Use the Iris dataset (X, y from Task 21)\n","# X is the full dataset\n","\n","# Train a PCA model with all components\n","pca_full = PCA(n_components=None) # n_components=None retains all components\n","pca_full.fit(X)\n","\n","# Calculate cumulative explained variance\n","cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n","\n","# Plot the cumulative explained variance\n","plt.figure(figsize=(8, 6))\n","plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n","plt.title('Task 30: Cumulative Explained Variance by Principal Components (Iris Dataset)')\n","plt.xlabel('Number of Principal Components')\n","plt.ylabel('Cumulative Explained Variance Ratio')\n","plt.grid(True)\n","plt.xticks(range(1, len(cumulative_variance) + 1))\n","plt.axhline(y=0.95, color='r', linestyle=':', label='95% explained variance')\n","plt.legend()\n","plt.show()\n","\n","print(f\"\\nTask 30: Explained Variance Ratio for each component (Iris):\")\n","for i, ratio in enumerate(pca_full.explained_variance_ratio_):\n","    print(f\"  Component {i+1}: {ratio:.4f}\")\n","\n","print(f\"Cumulative Explained Variance: {cumulative_variance}\")"],"metadata":{"id":"EmYyHbQlARzV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["31: Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare accuracy."],"metadata":{"id":"lsjud-PnA6Il"}},{"cell_type":"code","source":["# Load Iris dataset (if not already loaded)\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# KNN with uniform weights\n","knn_uniform = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n","knn_uniform.fit(X_train, y_train)\n","y_pred_uniform = knn_uniform.predict(X_test)\n","accuracy_uniform = accuracy_score(y_test, y_pred_uniform)\n","\n","# KNN with distance weights\n","knn_distance = KNeighborsClassifier(n_neighbors=5, weights='distance')\n","knn_distance.fit(X_train, y_train)\n","y_pred_distance = knn_distance.predict(X_test)\n","accuracy_distance = accuracy_score(y_test, y_pred_distance)\n","\n","print(f\"Task 31: KNN Accuracy (Weights='uniform'): {accuracy_uniform:.4f}\")\n","print(f\"Task 31: KNN Accuracy (Weights='distance'): {accuracy_distance:.4f}\")\n","\n","if accuracy_distance > accuracy_uniform:\n","    print(\"           Distance weighting performed better.\")\n","elif accuracy_uniform > accuracy_distance:\n","    print(\"           Uniform weighting performed better.\")\n","else:\n","    print(\"           Both weighting schemes performed equally well.\")"],"metadata":{"id":"zlubj5OEA7kO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["32: Train a KNN Regressor and analyze the effect of different K values on performance."],"metadata":{"id":"K8iW2w6rA9EL"}},{"cell_type":"code","source":["# Generate a synthetic regression dataset\n","X_reg, y_reg = make_regression(n_samples=100, n_features=1, noise=20, random_state=42)\n","X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n","    X_reg, y_reg, test_size=0.3, random_state=42\n",")\n","\n","k_values = range(1, 21) # Test K from 1 to 20\n","mse_scores = []\n","\n","for k in k_values:\n","    knn_regressor = KNeighborsRegressor(n_neighbors=k)\n","    knn_regressor.fit(X_train_reg, y_train_reg)\n","    y_pred_reg = knn_regressor.predict(X_test_reg)\n","    mse_scores.append(mean_squared_error(y_test_reg, y_pred_reg))\n","\n","plt.figure(figsize=(10, 6))\n","plt.plot(k_values, mse_scores, marker='o', linestyle='-')\n","plt.title('Task 32: Effect of K on KNN Regressor Performance (MSE)')\n","plt.xlabel('Number of Neighbors (K)')\n","plt.ylabel('Mean Squared Error (MSE)')\n","plt.xticks(k_values)\n","plt.grid(True)\n","plt.show()\n","\n","best_k_reg = k_values[np.argmin(mse_scores)]\n","min_mse = np.min(mse_scores)\n","print(f\"\\nTask 32: Best K for KNN Regressor (lowest MSE): {best_k_reg}\")\n","print(f\"Task 32: Minimum MSE observed: {min_mse:.4f}\")"],"metadata":{"id":"1v9hQXG7A-py"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["33: Implement KNN Imputation for handling missing values in a dataset."],"metadata":{"id":"R6scWAf8A_ne"}},{"cell_type":"code","source":["# Create a synthetic dataset with missing values\n","np.random.seed(42)\n","X_missing = np.array([\n","    [1.0, 2.0, np.nan],\n","    [3.0, np.nan, 5.0],\n","    [np.nan, 6.0, 7.0],\n","    [8.0, 9.0, 10.0],\n","    [11.0, 12.0, np.nan]\n","])\n","\n","print(f\"\\nTask 33: Original Dataset with Missing Values:\\n{X_missing}\")\n","\n","# Initialize KNNImputer\n","# n_neighbors specifies the number of neighboring samples to use for imputation\n","imputer = KNNImputer(n_neighbors=2)\n","\n","# Fit and transform the data\n","X_imputed = imputer.fit_transform(X_missing)\n","\n","print(f\"\\nTask 33: Dataset after KNN Imputation:\\n{X_imputed}\")"],"metadata":{"id":"7qUpRHTfBBJ6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["34: Train a PCA model and visualize the data projection onto the first two principal components."],"metadata":{"id":"ghukjtUfBCPA"}},{"cell_type":"code","source":["# Load Iris dataset (X, y)\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Apply PCA to reduce to 2 components\n","pca_2d = PCA(n_components=2)\n","X_pca_2d = pca_2d.fit_transform(X)\n","\n","# Visualize the projection\n","plt.figure(figsize=(8, 6))\n","sns.scatterplot(x=X_pca_2d[:, 0], y=X_pca_2d[:, 1], hue=y, palette='viridis', legend='full')\n","plt.title('Task 34: Data Projection onto First Two Principal Components (Iris)')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"BJh8ruO1BDbb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["35: Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance."],"metadata":{"id":"5Ew-acQvBFu5"}},{"cell_type":"code","source":["# Load Iris dataset (X, y)\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# KNN with KDTree algorithm\n","knn_kdtree = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\n","knn_kdtree.fit(X_train, y_train)\n","y_pred_kdtree = knn_kdtree.predict(X_test)\n","accuracy_kdtree = accuracy_score(y_test, y_pred_kdtree)\n","\n","# KNN with BallTree algorithm\n","knn_balltree = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n","knn_balltree.fit(X_train, y_train)\n","y_pred_balltree = knn_balltree.predict(X_test)\n","accuracy_balltree = accuracy_score(y_test, y_pred_balltree)\n","\n","print(f\"\\nTask 35: KNN Accuracy (Algorithm='kd_tree'): {accuracy_kdtree:.4f}\")\n","print(f\"Task 35: KNN Accuracy (Algorithm='ball_tree'): {accuracy_balltree:.4f}\")\n","\n","if accuracy_kdtree > accuracy_balltree:\n","    print(\"           KD Tree performed slightly better.\")\n","elif accuracy_balltree > accuracy_kdtree:\n","    print(\"           Ball Tree performed slightly better.\")\n","else:\n","    print(\"           Both algorithms resulted in similar accuracy for this dataset.\")\n","print(\"           Note: The primary difference is often computational efficiency for large datasets, not accuracy.\")"],"metadata":{"id":"LuBf4aQcBHDE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["36: Train a PCA model on a high-dimensional dataset and visualize the Scree plot."],"metadata":{"id":"EBy1uuCgBICn"}},{"cell_type":"code","source":["# Generate a high-dimensional synthetic dataset\n","X_high_dim, _ = make_classification(n_samples=100, n_features=50, random_state=42)\n","\n","# Train a PCA model with all components\n","pca_scree = PCA(n_components=None)\n","pca_scree.fit(X_high_dim)\n","\n","# Get explained variance for each component\n","explained_variance = pca_scree.explained_variance_\n","\n","# Plot the Scree plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n","plt.title('Task 36: Scree Plot for High-Dimensional Data')\n","plt.xlabel('Principal Component Number')\n","plt.ylabel('Eigenvalue (Explained Variance)')\n","plt.grid(True)\n","plt.show()\n","\n","print(f\"\\nTask 36: Explained Variance for first 5 components of high-dimensional data:\")\n","for i in range(min(5, len(explained_variance))):\n","    print(f\"  Component {i+1}: {explained_variance[i]:.4f}\")"],"metadata":{"id":"fO2HVAkJBJhb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["37: Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score."],"metadata":{"id":"Z7sDEj_MBKiE"}},{"cell_type":"code","source":["# Load Iris dataset (X, y)\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Train a KNN Classifier\n","knn_metrics = KNeighborsClassifier(n_neighbors=5)\n","knn_metrics.fit(X_train, y_train)\n","y_pred_metrics = knn_metrics.predict(X_test)\n","\n","# Print classification report\n","print(f\"\\nTask 37: Classification Report for KNN Classifier:\\n\")\n","print(classification_report(y_test, y_pred_metrics, target_names=iris.target_names))\n","\n","# Optionally, print individual scores\n","print(f\"Overall Precision (macro avg): {precision_score(y_test, y_pred_metrics, average='macro'):.4f}\")\n","print(f\"Overall Recall (macro avg): {recall_score(y_test, y_pred_metrics, average='macro'):.4f}\")\n","print(f\"Overall F1-Score (macro avg): {f1_score(y_test, y_pred_metrics, average='macro'):.4f}\")"],"metadata":{"id":"szuldAqCBLsw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["38: Train a PCA model and analyze the effect of different numbers of components on accuracy."],"metadata":{"id":"h5FKiPyfBMue"}},{"cell_type":"code","source":["# Load Iris dataset (X, y)\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","n_components_list = range(1, X.shape[1] + 1) # From 1 to max features\n","accuracy_pca_effect = []\n","\n","for n_comp in n_components_list:\n","    pca_effect = PCA(n_components=n_comp)\n","    X_train_pca_effect = pca_effect.fit_transform(X_train)\n","    X_test_pca_effect = pca_effect.transform(X_test)\n","\n","    knn_pca_effect = KNeighborsClassifier(n_neighbors=5)\n","    knn_pca_effect.fit(X_train_pca_effect, y_train)\n","    y_pred_pca_effect = knn_pca_effect.predict(X_test_pca_effect)\n","    accuracy_pca_effect.append(accuracy_score(y_test, y_pred_pca_effect))\n","\n","plt.figure(figsize=(10, 6))\n","plt.plot(n_components_list, accuracy_pca_effect, marker='o', linestyle='-')\n","plt.title('Task 38: Effect of Number of PCA Components on KNN Accuracy')\n","plt.xlabel('Number of Principal Components')\n","plt.ylabel('KNN Accuracy')\n","plt.xticks(n_components_list)\n","plt.grid(True)\n","plt.show()\n","\n","optimal_n_comp = n_components_list[np.argmax(accuracy_pca_effect)]\n","max_accuracy_pca = np.max(accuracy_pca_effect)\n","print(f\"\\nTask 38: Optimal number of PCA components (max accuracy): {optimal_n_comp}\")\n","print(f\"Task 38: Maximum accuracy with PCA: {max_accuracy_pca:.4f}\")"],"metadata":{"id":"6Q8pCy_UBN9h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["39: Train a KNN Classifier with different leaf_size values and compare accuracy."],"metadata":{"id":"CYjSUiOaBQoC"}},{"cell_type":"code","source":["# Load Iris dataset (X, y)\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","leaf_sizes = [1, 10, 30, 50] # Common leaf_size values\n","accuracy_leaf_size = []\n","\n","for ls in leaf_sizes:\n","    knn_leaf = KNeighborsClassifier(n_neighbors=5, leaf_size=ls)\n","    knn_leaf.fit(X_train, y_train)\n","    y_pred_leaf = knn_leaf.predict(X_test)\n","    accuracy_leaf_size.append(accuracy_score(y_test, y_pred_leaf))\n","    print(f\"Task 39: KNN Accuracy (leaf_size={ls}): {accuracy_leaf_size[-1]:.4f}\")\n","\n","# Note: For small datasets like Iris, leaf_size typically has minimal to no effect on accuracy\n","# It primarily impacts the speed of neighbor queries for very large datasets."],"metadata":{"id":"3pIhTXpcBRB4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["40: Train a PCA model and visualize how data points are transformed before and after PCA."],"metadata":{"id":"dGqUt1maBR-6"}},{"cell_type":"code","source":["# Generate a 2D synthetic dataset (e.g., elongated blob)\n","X_orig, y_orig = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2,\n","                                     n_clusters_per_class=1, random_state=42, class_sep=1.5)\n","\n","# Stretch the data to make PCA more obvious\n","X_orig[:, 0] = X_orig[:, 0] * 3\n","\n","# Plot original data\n","plt.figure(figsize=(12, 6))\n","plt.subplot(1, 2, 1)\n","sns.scatterplot(x=X_orig[:, 0], y=X_orig[:, 1], hue=y_orig, palette='viridis', legend='full')\n","plt.title('Task 40: Original Data (2 Features)')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.grid(True)\n","\n","# Apply PCA\n","pca_transform = PCA(n_components=2)\n","X_transformed = pca_transform.fit_transform(X_orig)\n","\n","# Plot transformed data\n","plt.subplot(1, 2, 2)\n","sns.scatterplot(x=X_transformed[:, 0], y=X_transformed[:, 1], hue=y_orig, palette='viridis', legend='full')\n","plt.title('Task 40: PCA Transformed Data (2 Principal Components)')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"\\nTask 40: Original data has its variance distributed along original axes.\")\n","print(\"         PCA transformed data has its variance aligned with the new principal components,\")\n","print(f\"         and PC1 typically captures the most variance: {pca_transform.explained_variance_ratio_[0]:.4f}\")"],"metadata":{"id":"c9MdROw8BTej"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["41: Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report."],"metadata":{"id":"lphbix-_BUZn"}},{"cell_type":"code","source":["# Load Wine dataset\n","wine = load_wine()\n","X_wine, y_wine = wine.data, wine.target\n","\n","# Split data\n","X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(\n","    X_wine, y_wine, test_size=0.3, random_state=42, stratify=y_wine # Stratify for balanced classes\n",")\n","\n","# Scale the data (important for KNN)\n","scaler_wine = StandardScaler()\n","X_train_wine_scaled = scaler_wine.fit_transform(X_train_wine)\n","X_test_wine_scaled = scaler_wine.transform(X_test_wine)\n","\n","# Train a KNN Classifier\n","knn_wine = KNeighborsClassifier(n_neighbors=5)\n","knn_wine.fit(X_train_wine_scaled, y_train_wine)\n","y_pred_wine = knn_wine.predict(X_test_wine_scaled)\n","\n","# Print classification report\n","print(f\"\\nTask 41: Classification Report for KNN Classifier on Wine Dataset:\\n\")\n","print(classification_report(y_test_wine, y_pred_wine, target_names=wine.target_names))"],"metadata":{"id":"Zm8Zy9EkBVuC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["42: Train a KNN Regressor and analyze the effect of different distance metrics on prediction error."],"metadata":{"id":"BlBcFOJxBWvy"}},{"cell_type":"code","source":["# Use the synthetic regression dataset (X_reg, y_reg from Task 32)\n","# X_train_reg, X_test_reg, y_train_reg, y_test_reg are already defined\n","\n","# KNN Regressor with Euclidean distance\n","knn_reg_euclidean = KNeighborsRegressor(n_neighbors=5, metric='euclidean')\n","knn_reg_euclidean.fit(X_train_reg, y_train_reg)\n","y_pred_reg_euclidean = knn_reg_euclidean.predict(X_test_reg)\n","mse_euclidean_reg = mean_squared_error(y_test_reg, y_pred_reg_euclidean)\n","\n","# KNN Regressor with Manhattan distance\n","knn_reg_manhattan = KNeighborsRegressor(n_neighbors=5, metric='manhattan')\n","knn_reg_manhattan.fit(X_train_reg, y_train_reg)\n","y_pred_reg_manhattan = knn_reg_manhattan.predict(X_test_reg)\n","mse_manhattan_reg = mean_squared_error(y_test_reg, y_pred_reg_manhattan)\n","\n","print(f\"\\nTask 42: KNN Regressor MSE (Euclidean Distance): {mse_euclidean_reg:.4f}\")\n","print(f\"Task 42: KNN Regressor MSE (Manhattan Distance): {mse_manhattan_reg:.4f}\")\n","\n","if mse_euclidean_reg < mse_manhattan_reg:\n","    print(\"           Euclidean distance performed better (lower MSE).\")\n","elif mse_manhattan_reg < mse_euclidean_reg:\n","    print(\"           Manhattan distance performed better (lower MSE).\")\n","else:\n","    print(\"           Both distances performed equally well.\")"],"metadata":{"id":"q6-t4AC4BYD-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["43: Train a KNN Classifier and evaluate using ROC-AUC score."],"metadata":{"id":"dS0NUCKWBY-C"}},{"cell_type":"code","source":["# Load Iris dataset\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Convert to a binary classification problem for ROC-AUC\n","# Class 0 vs. (Class 1 or 2)\n","y_binary = (y == 0).astype(int)\n","\n","# Split data\n","X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(\n","    X, y_binary, test_size=0.3, random_state=42, stratify=y_binary\n",")\n","\n","# Scale the data\n","scaler_bin = StandardScaler()\n","X_train_bin_scaled = scaler_bin.fit_transform(X_train_bin)\n","X_test_bin_scaled = scaler_bin.transform(X_test_bin)\n","\n","# Train a KNN Classifier (need probabilities for ROC-AUC)\n","knn_roc = KNeighborsClassifier(n_neighbors=5)\n","knn_roc.fit(X_train_bin_scaled, y_train_bin)\n","\n","# Get probability estimates for the positive class (class 1)\n","y_prob = knn_roc.predict_proba(X_test_bin_scaled)[:, 1]\n","\n","# Calculate ROC-AUC score\n","auc_score = roc_auc_score(y_test_bin, y_prob)\n","print(f\"\\nTask 43: KNN Classifier ROC-AUC Score: {auc_score:.4f}\")\n","\n","# Plot ROC curve\n","fpr, tpr, thresholds = roc_curve(y_test_bin, y_prob)\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc_score:.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Task 43: Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc=\"lower right\")\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"_WS2vQk3BaHB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["44: Train a PCA model and visualize the variance captured by each principal component."],"metadata":{"id":"kmEZV6UEBbQf"}},{"cell_type":"code","source":["# Load Iris dataset (X, y)\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Train a PCA model with all components\n","pca_variance_plot = PCA(n_components=None)\n","pca_variance_plot.fit(X)\n","\n","explained_variance_ratio = pca_variance_plot.explained_variance_ratio_\n","\n","# Plot the explained variance captured by each component\n","plt.figure(figsize=(8, 6))\n","plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n","plt.title('Task 44: Variance Captured by Each Principal Component (Iris)')\n","plt.xlabel('Principal Component Number')\n","plt.ylabel('Explained Variance Ratio')\n","plt.xticks(range(1, len(explained_variance_ratio) + 1))\n","plt.grid(axis='y')\n","plt.show()\n","\n","print(f\"\\nTask 44: Explained Variance Ratio for each PCA component (Iris):\")\n","for i, ratio in enumerate(explained_variance_ratio):\n","    print(f\"  Component {i+1}: {ratio:.4f}\")"],"metadata":{"id":"-VIOXcUtBca8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["45: Train a KNN Classifier and perform feature selection before training."],"metadata":{"id":"OzWzeu-GBdTl"}},{"cell_type":"code","source":["# Load Iris dataset (X, y)\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# --- KNN without Feature Selection ---\n","knn_no_fs = KNeighborsClassifier(n_neighbors=5)\n","knn_no_fs.fit(X_train, y_train)\n","y_pred_no_fs = knn_no_fs.predict(X_test)\n","accuracy_no_fs = accuracy_score(y_test, y_pred_no_fs)\n","print(f\"\\nTask 45: KNN Accuracy (Without Feature Selection): {accuracy_no_fs:.4f}\")\n","\n","# --- KNN with Feature Selection ---\n","# Select top 2 features using f_classif (ANOVA F-value)\n","selector = SelectKBest(f_classif, k=2) # Choose top 2 features\n","X_train_selected = selector.fit_transform(X_train, y_train)\n","X_test_selected = selector.transform(X_test)\n","\n","# Print selected features (indices)\n","selected_features_indices = selector.get_support(indices=True)\n","print(f\"Task 45: Selected Feature Indices: {selected_features_indices}\")\n","print(f\"Task 45: Corresponding Feature Names: {[iris.feature_names[i] for i in selected_features_indices]}\")\n","\n","\n","# Train KNN on selected features\n","knn_fs = KNeighborsClassifier(n_neighbors=5)\n","knn_fs.fit(X_train_selected, y_train)\n","y_pred_fs = knn_fs.predict(X_test_selected)\n","accuracy_fs = accuracy_score(y_test, y_pred_fs)\n","print(f\"Task 45: KNN Accuracy (With Feature Selection, k=2): {accuracy_fs:.4f}\")\n","\n","if accuracy_fs > accuracy_no_fs:\n","    print(\"           Feature selection improved accuracy.\")\n","elif accuracy_no_fs > accuracy_fs:\n","    print(\"           Feature selection reduced accuracy.\")\n","else:\n","    print(\"           Feature selection had no effect on accuracy.\")"],"metadata":{"id":"L0yoVppJBeey"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["46: Train a PCA model and visualize the data reconstruction error after reducing dimensions."],"metadata":{"id":"tsQW6uCpBfbv"}},{"cell_type":"code","source":["# Generate a synthetic 2D dataset\n","X_rec, y_rec = make_classification(n_samples=100, n_features=2, n_informative=2,\n","                                  n_redundant=0, n_clusters_per_class=1, random_state=42, class_sep=1.5)\n","\n","# Reduce dimensions to 1 component, then reconstruct\n","pca_reconstruction = PCA(n_components=1)\n","X_reduced = pca_reconstruction.fit_transform(X_rec)\n","X_reconstructed = pca_reconstruction.inverse_transform(X_reduced)\n","\n","# Calculate reconstruction error (Mean Squared Error between original and reconstructed)\n","reconstruction_error = mean_squared_error(X_rec, X_reconstructed)\n","print(f\"\\nTask 46: Data Reconstruction Error (MSE with 1 component): {reconstruction_error:.4f}\")\n","\n","# Visualize original vs. reconstructed data\n","plt.figure(figsize=(10, 7))\n","sns.scatterplot(x=X_rec[:, 0], y=X_rec[:, 1], color='blue', label='Original Data', alpha=0.6)\n","sns.scatterplot(x=X_reconstructed[:, 0], y=X_reconstructed[:, 1], color='red', marker='x', s=100, label='Reconstructed Data (1 PC)', alpha=0.8)\n","plt.title('Task 46: Original vs. Reconstructed Data after PCA (1 Component)')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"cjxEtjoeBg-f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["47: Train a KNN Classifier and visualize the decision boundary."],"metadata":{"id":"HErXOLFBBiKj"}},{"cell_type":"code","source":["# Use only the first two features from Iris for visualization\n","X_2d_db = X[:, :2] # Sepal Length, Sepal Width\n","y_2d_db = y\n","\n","# Split data for 2D visualization\n","X_train_2d_db, X_test_2d_db, y_train_2d_db, y_test_2d_db = train_test_split(\n","    X_2d_db, y_2d_db, test_size=0.3, random_state=42\n",")\n","\n","# Define a color map for the classes\n","cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n","cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n","\n","# Train a KNN Classifier\n","knn_db = KNeighborsClassifier(n_neighbors=5)\n","knn_db.fit(X_train_2d_db, y_train_2d_db)\n","\n","# Plot the decision boundary\n","x_min, x_max = X_2d_db[:, 0].min() - 1, X_2d_db[:, 0].max() + 1\n","y_min, y_max = X_2d_db[:, 1].min() - 1, X_2d_db[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n","                     np.arange(y_min, y_max, 0.02))\n","Z = knn_db.predict(np.c_[xx.ravel(), yy.ravel()])\n","Z = Z.reshape(xx.shape)\n","\n","plt.figure(figsize=(8, 6))\n","plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')\n","\n","# Plot training and test points\n","plt.scatter(X_train_2d_db[:, 0], X_train_2d_db[:, 1], c=y_train_2d_db, cmap=cmap_bold,\n","            edgecolor='k', s=20, label='Training points')\n","plt.scatter(X_test_2d_db[:, 0], X_test_2d_db[:, 1], c=y_test_2d_db, cmap=cmap_bold,\n","            edgecolor='k', s=60, marker='X', label='Test points')\n","plt.xlim(xx.min(), xx.max())\n","plt.ylim(yy.min(), yy.max())\n","plt.title('Task 47: KNN Classifier Decision Boundary')\n","plt.xlabel(iris.feature_names[0])\n","plt.ylabel(iris.feature_names[1])\n","plt.legend()\n","plt.show()"],"metadata":{"id":"gUQIDceNBjYF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["48: Train a PCA model and analyze the effect of different numbers of components on data variance."],"metadata":{"id":"UlUkujokBklW"}},{"cell_type":"code","source":["# Load Iris dataset (X, y)\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","print(f\"\\nTask 48: Effect of different numbers of PCA components on total explained variance:\")\n","\n","for n_comp in range(1, X.shape[1] + 1):\n","    pca_var_analysis = PCA(n_components=n_comp)\n","    pca_var_analysis.fit(X)\n","    total_explained_variance = np.sum(pca_var_analysis.explained_variance_ratio_)\n","    print(f\"  With {n_comp} component(s): Total Explained Variance = {total_explained_variance:.4f}\")\n","\n","# Also show the cumulative plot from Task 30 again for visual understanding\n","# (Code duplicated for completeness of this task's output)\n","pca_full_reprise = PCA(n_components=None)\n","pca_full_reprise.fit(X)\n","cumulative_variance_reprise = np.cumsum(pca_full_reprise.explained_variance_ratio_)\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(range(1, len(cumulative_variance_reprise) + 1), cumulative_variance_reprise, marker='o', linestyle='--')\n","plt.title('Task 48: Cumulative Explained Variance by Principal Components (Iris Dataset)')\n","plt.xlabel('Number of Principal Components')\n","plt.ylabel('Cumulative Explained Variance Ratio')\n","plt.grid(True)\n","plt.xticks(range(1, len(cumulative_variance_reprise) + 1))\n","plt.axhline(y=0.95, color='r', linestyle=':', label='95% explained variance')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"drHkSTKOBlwF"},"execution_count":null,"outputs":[]}]}